{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maorisraelii/maorisraelii-twitter-sentiment-analysis/blob/main/twitter_sentiment_analysis_and_Stock_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsGCARTn1FvP"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "#using bert, worldcloud, sentiment analysis on time.\n",
        "\n",
        "# Define the ticker symbols for all companies in the S&P 500 index\n",
        "sp500_tickers = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]['Symbol'].tolist()\n",
        "\n",
        "\n",
        "# Define the start and end dates for the data\n",
        "start_date = '2019-10-01'\n",
        "end_date = '2020-12-31'\n",
        "\n",
        "# Create an empty dictionary to store the dataframes for each company's historical data\n",
        "company_data_2019_2020 = {}\n",
        "\n",
        "# Iterate over each ticker symbol and download the historical data using yfinance\n",
        "for ticker_symbol in sp500_tickers:\n",
        "    try:\n",
        "        company_data_2019_2020[ticker_symbol] = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading data for {ticker_symbol}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQQOXyjd1T0Z"
      },
      "outputs": [],
      "source": [
        "#Repeat for other dates\n",
        "start_date = '2022-01-01'\n",
        "end_date = '2022-12-31'\n",
        "\n",
        "company_data_2022 = {}\n",
        "for ticker_symbol in sp500_tickers:\n",
        "    try:\n",
        "        company_data_2022[ticker_symbol] = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading data for {ticker_symbol}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5W-dS9JA13w"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "musk = pd.read_csv('/content/drive/MyDrive/Musk(2014-2019).csv')\n",
        "biden = pd.read_csv('/content/drive/MyDrive/Biden(2007-2020).csv')\n",
        "trump = pd.read_csv('/content/drive/MyDrive/Trump(2017-2021).csv',error_bad_lines=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_gOZR3fW834"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Define regular expressions to match links and emojis\n",
        "link_pattern = r'https?://\\S+'\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "# Remove tweets containing only links or emojis\n",
        "musk['tweet'] = musk['tweet'].fillna('')  # Replace NaN values with empty string\n",
        "musk = musk[~musk['tweet'].str.match(link_pattern)]\n",
        "musk = musk[~musk['tweet'].str.contains(emoji_pattern)]\n",
        "\n",
        "biden['tweet'] = biden['tweet'].fillna('')  # Replace NaN values with empty string\n",
        "biden = biden[~biden['tweet'].str.match(link_pattern)]\n",
        "biden = biden[~biden['tweet'].str.contains(emoji_pattern)]\n",
        "\n",
        "trump['tweet'] = trump['tweet'].fillna('')  # Replace NaN values with empty string\n",
        "trump = trump[~trump['tweet'].str.match(link_pattern)]\n",
        "trump = trump[~trump['tweet'].str.contains(emoji_pattern)]\n",
        "\n",
        "# Removing rows and columns with NaN / empty values\n",
        "musk = musk.dropna(axis=0, how='all')\n",
        "musk = musk.dropna(axis=1, how='all')\n",
        "musk = musk.reset_index(drop=True)\n",
        "musk.dropna(subset=['date'], inplace=True)\n",
        "\n",
        "biden = biden.dropna(axis=0, how='all')\n",
        "biden = biden.dropna(axis=1, how='all')\n",
        "biden = biden.reset_index(drop=True)\n",
        "biden.dropna(subset=['time'], inplace=True)\n",
        "\n",
        "trump = trump.dropna(axis=0, how='all')\n",
        "trump = trump.dropna(axis=1, how='all')\n",
        "trump = trump.reset_index(drop=True)\n",
        "trump.dropna(subset=['time'], inplace=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQFmv6X9Xhaz"
      },
      "outputs": [],
      "source": [
        "trump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph2mS_E0XtOQ"
      },
      "outputs": [],
      "source": [
        "musk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "biden"
      ],
      "metadata": {
        "id": "mbw6Rn5r-Ckn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfv9oMexm6EM"
      },
      "outputs": [],
      "source": [
        "trump['time'] = pd.to_datetime(trump['time'])\n",
        "start_date = pd.to_datetime('2019-10-01')\n",
        "end_date = pd.to_datetime('2020-12-31')\n",
        "trump2019_2020 = trump[(trump['time'] >= start_date) & (trump['time'] <= end_date)]\n",
        "trump2019_2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq_4EIfhqGuN"
      },
      "outputs": [],
      "source": [
        "biden['time'] = pd.to_datetime(biden['time'])\n",
        "start_date = pd.to_datetime('2019-10-01')\n",
        "end_date = pd.to_datetime('2020-12-31')\n",
        "biden2019_2020 = biden[(biden['time'] >= start_date) & (biden['time'] <= end_date)]\n",
        "biden2019_2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxMy_qPxqV5D"
      },
      "outputs": [],
      "source": [
        "musk['date'] = pd.to_datetime(musk['date'])\n",
        "start_date = pd.to_datetime('2019-10-01')\n",
        "end_date = pd.to_datetime('2020-12-31')\n",
        "musk2019_2020 = musk[(musk['date'] >= start_date) & (musk['date'] <= end_date)]\n",
        "musk2019_2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJI3g7Gmqvsg"
      },
      "outputs": [],
      "source": [
        "# Replace emojis with a describing text\n",
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "def convert_emojis_to_text(text):\n",
        "    return emoji.demojize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp5sn1ecr9mA"
      },
      "outputs": [],
      "source": [
        "musk2019_2020.loc[:, 'tweet'] = musk2019_2020['tweet'].apply(convert_emojis_to_text)\n",
        "musk2019_2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5Ii4DlDu9Ic"
      },
      "outputs": [],
      "source": [
        "biden2019_2020.loc[:, 'tweet'] = biden2019_2020['tweet'].apply(convert_emojis_to_text)\n",
        "biden2019_2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCOs01hau93n"
      },
      "outputs": [],
      "source": [
        "trump2019_2020.loc[:, 'tweet'] = trump2019_2020['tweet'].apply(convert_emojis_to_text)\n",
        "trump2019_2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEGxKehyL8tT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to generate word cloud for a given data frame\n",
        "def generate_word_cloud(df):\n",
        "    # Extract the \"tweet\" column\n",
        "    tweets = df['tweet'].astype(str)  # Convert to string type\n",
        "\n",
        "    # Join all the tweets into a single string\n",
        "    all_tweets = ' '.join(tweets)\n",
        "\n",
        "    # Set the stopwords to ignore in the word cloud\n",
        "    stopwords = set(STOPWORDS)\n",
        "    stopwords.update(['the', 'a', 'in', 'with', 'to', 'an', 'at', 'on', 'off', 'with', 'without', 'by', 'RT', \n",
        "                      'http','https','amp','will', 'am', 'are','realDonaldTrump','Thank','President','t','co','s','elonmusk','Trump'])\n",
        "\n",
        "    # Generate the word cloud\n",
        "    wordcloud = WordCloud(stopwords=stopwords).generate(all_tweets)\n",
        "\n",
        "    # Display the word cloud\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Generate word cloud for the variable data\n",
        "print(\"Trump's the most used words\")\n",
        "generate_word_cloud(trump2019_2020)\n",
        "print('\\n\\n')\n",
        "\n",
        "print(\"Musk's the most used words\")\n",
        "generate_word_cloud(musk2019_2020)\n",
        "print('\\n\\n')\n",
        "\n",
        "print(\"Biden's the most used words\")\n",
        "generate_word_cloud(biden2019_2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqNaOq_29kak"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Function to perform sentiment analysis using TextBlob\n",
        "def perform_sentiment_analysis(df):\n",
        "    analyzed_df = df.copy()\n",
        "    analyzed_df['sentiment'] = ''\n",
        "    analyzed_df['subjectivity'] = ''\n",
        "    analyzed_df['polarity'] = ''\n",
        "\n",
        "    for index, row in analyzed_df.iterrows():\n",
        "        tweet = row['tweet']\n",
        "        sentiment, subjectivity, polarity = get_sentiment_label(TextBlob(tweet).sentiment)\n",
        "        analyzed_df.at[index, 'sentiment'] = sentiment\n",
        "        analyzed_df.at[index, 'subjectivity'] = subjectivity\n",
        "        analyzed_df.at[index, 'polarity'] = polarity\n",
        "\n",
        "    return analyzed_df\n",
        "\n",
        "# Function to get sentiment label, subjectivity, and polarity based on sentiment score\n",
        "def get_sentiment_label(sentiment):\n",
        "    polarity = sentiment.polarity\n",
        "    subjectivity = sentiment.subjectivity\n",
        "    \n",
        "    if polarity > 0:\n",
        "        sentiment_label = 'positive'\n",
        "    elif polarity < 0:\n",
        "        sentiment_label = 'negative'\n",
        "    else:\n",
        "        sentiment_label = 'neutral'\n",
        "    \n",
        "    return sentiment_label, subjectivity, polarity\n",
        "\n",
        "# Perform sentiment analysis on Musk's tweets\n",
        "analyzed_musk2019_2020 = perform_sentiment_analysis(musk2019_2020)\n",
        "\n",
        "# Perform sentiment analysis on Biden's tweets\n",
        "analyzed_biden2019_2020 = perform_sentiment_analysis(biden2019_2020)\n",
        "\n",
        "# Perform sentiment analysis on Trump's tweets\n",
        "analyzed_trump2019_2020 = perform_sentiment_analysis(trump2019_2020)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyzed_musk2019_2020"
      ],
      "metadata": {
        "id": "iu4J57dA-HKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzed_trump2019_2020"
      ],
      "metadata": {
        "id": "clKxYrU3-HBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy4qn_pLBkyW"
      },
      "outputs": [],
      "source": [
        "analyzed_biden2019_2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY2DH87IEHo1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to set color based on sentiment\n",
        "def get_color(sentiment):\n",
        "    if sentiment == 'negative':\n",
        "        return 'red'\n",
        "    elif sentiment == 'neutral':\n",
        "        return 'gray'\n",
        "    elif sentiment == 'positive':\n",
        "        return 'blue'\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot for Musk's tweets\n",
        "for i, row in analyzed_musk2019_2020.iterrows():\n",
        "    polarity = row['polarity']\n",
        "    subjectivity = row['subjectivity']\n",
        "    sentiment = row['sentiment']\n",
        "    color = get_color(sentiment)\n",
        "    plt.scatter(polarity, subjectivity, color=color)\n",
        "\n",
        "# Plot for Trump's tweets\n",
        "for i, row in analyzed_trump2019_2020.iterrows():\n",
        "    polarity = row['polarity']\n",
        "    subjectivity = row['subjectivity']\n",
        "    sentiment = row['sentiment']\n",
        "    color = get_color(sentiment)\n",
        "    plt.scatter(polarity, subjectivity, color=color)\n",
        "\n",
        "# Plot for Biden's tweets\n",
        "for i, row in analyzed_biden2019_2020.iterrows():\n",
        "    polarity = row['polarity']\n",
        "    subjectivity = row['subjectivity']\n",
        "    sentiment = row['sentiment']\n",
        "    color = get_color(sentiment)\n",
        "    plt.scatter(polarity, subjectivity, color=color)\n",
        "\n",
        "# Set labels and title\n",
        "plt.title('Sentiment Analysis of Tweets')\n",
        "plt.xlabel('Polarity')\n",
        "plt.ylabel('Subjectivity')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the \"time\" column to datetime\n",
        "analyzed_musk2019_2020['time'] = pd.to_datetime(analyzed_musk2019_2020['time'])\n",
        "\n",
        "# Group by the date and calculate the average sentiment for each day\n",
        "day_grouped_musk2019_2020 = analyzed_musk2019_2020.groupby(analyzed_musk2019_2020['time'].dt.date)['polarity'].mean()\n",
        "\n",
        "day_grouped_musk2019_2020"
      ],
      "metadata": {
        "id": "OFE0YFYerMHb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "706855db-bf90-4bee-bbdb-f6eeb0cff32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-23ec03ed5fed>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert the \"time\" column to datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0manalyzed_musk2019_2020\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzed_musk2019_2020\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Group by the date and calculate the average sentiment for each day\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mday_grouped_musk2019_2020\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzed_musk2019_2020\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzed_musk2019_2020\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'polarity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the \"time\" column to datetime\n",
        "analyzed_trump2019_2020['time'] = pd.to_datetime(analyzed_trump2019_2020['time'])\n",
        "\n",
        "# Group by the date and calculate the average sentiment for each day\n",
        "day_grouped_trump2019_2020 = analyzed_trump2019_2020.groupby(analyzed_trump2019_2020['time'].dt.date)['polarity'].mean()\n",
        "\n",
        "day_grouped_trump2019_2020"
      ],
      "metadata": {
        "id": "SqVvuuHQAM1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the \"time\" column to datetime\n",
        "analyzed_biden2019_2020['time'] = pd.to_datetime(analyzed_biden2019_2020['time'])\n",
        "\n",
        "# Group by the date and calculate the average sentiment for each day\n",
        "day_grouped_biden2019_2020 = analyzed_biden2019_2020.groupby(analyzed_biden2019_2020['time'].dt.date)['polarity'].mean()\n",
        "\n",
        "day_grouped_biden2019_2020"
      ],
      "metadata": {
        "id": "5v8DVnPoAM_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_data_2019_2020[\"ZION\"]['biden'] = day_grouped_biden2019_2020"
      ],
      "metadata": {
        "id": "rfe7_KNQOYwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_data_2019_2020[\"ZION\"]['biden'] = company_data_2019_2020[\"ZION\"]['biden'].fillna(0)\n",
        "company_data_2019_2020[\"ZION\"]"
      ],
      "metadata": {
        "id": "Bk5bS9oSPK8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\n",
        "    'Open',\n",
        "    'High', 'Low',\n",
        "    'Close',\n",
        "    'Adj Close',\n",
        "    'biden'\n",
        "        ]\n",
        "#Date and volume columns are not used in training.\n",
        "print(cols)\n",
        "\n",
        "#New dataframe with only training data - 5 columns\n",
        "df = company_data_2019_2020[\"ZION\"]\n",
        "df_for_training = df[cols].astype(float)\n",
        "# df_for_training.index = df['Date']\n",
        "df_for_training"
      ],
      "metadata": {
        "id": "rI2-DasRUSC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "# Data scaling for LSTM because uses sigmoid and tanh that are sensitive to magnitude\n",
        "scaler = MinMaxScaler()\n",
        "scaler = scaler.fit(df_for_training)\n",
        "df_for_training_scaled = scaler.transform(df_for_training)\n",
        "\n",
        "scaler_for_inference = MinMaxScaler()\n",
        "scaler_for_inference.fit_transform(df_for_training.loc[:,['Open','Adj Close']])\n",
        "\n",
        "df_for_training_scaled"
      ],
      "metadata": {
        "id": "jGksEZnCLjGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features.\n",
        "#Empty lists to be populated using formatted training data\n",
        "trainX = []\n",
        "trainY = []\n",
        "\n",
        "n_future = 1   # Number of days we want to look into the future based on the past days.\n",
        "n_past = 5  # Number of past days we want to use to predict the future.\n",
        "\n",
        "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
        "#In my example, my df_for_training_scaled has a shape (12823, 5)\n",
        "#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
        "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
        "    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
        "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future,[0,-2]])\n",
        "\n",
        "trainX, trainY = np.array(trainX), np.array(trainY)\n",
        "\n",
        "print('TrainX shape = {}.'.format(trainX.shape))\n",
        "print('TrainY shape = {}.'.format(trainY.shape))"
      ],
      "metadata": {
        "id": "yiP-E4TlLum8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Train test split for LSTM\n",
        "X_train_lstm_without_twitter, X_test_lstm_without_twitter, y_train_lstm_without_twitter, y_test_lstm_without_twitter = train_test_split(trainX[:,:,:-1], trainY, test_size=0.2, shuffle=False)\n",
        "\n",
        "X_train_lstm_twitter, X_test_lstm_twitter, y_train_lstm_twitter, y_test_lstm_twitter = train_test_split(trainX, trainY, test_size=0.2, shuffle=False)\n",
        "\n",
        "X_train_lstm_without_twitter.shape,X_train_lstm_twitter.shape"
      ],
      "metadata": {
        "id": "OmRNuqF8L7mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Train validation split for LSTM\n",
        "X_train_lstm_without_twitter, X_val_lstm_without_twitter, y_train_lstm_without_twitter, y_val_lstm_without_twitter = train_test_split(X_train_lstm_without_twitter, y_train_lstm_without_twitter, test_size=0.1, shuffle=False)\n",
        "\n",
        "X_train_lstm_twitter, X_val_lstm_twitter, y_train_lstm_twitter, y_val_lstm_twitter = train_test_split(X_train_lstm_twitter, y_train_lstm_twitter, test_size=0.1, shuffle=False)\n",
        "\n",
        "X_train_lstm_without_twitter.shape,X_train_lstm_twitter.shape"
      ],
      "metadata": {
        "id": "uNO8HmRSMLlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model architecture\n",
        "\n",
        "def build_model(input_shape):\n",
        "    tf.random.set_seed(seed)\n",
        "    cnn_lstm_model = Sequential()\n",
        "\n",
        "    cnn_lstm_model.add(Conv1D(filters=128, kernel_size=2, strides=1, padding='valid', input_shape=input_shape))\n",
        "    cnn_lstm_model.add(MaxPooling1D(pool_size=2, strides=2))\n",
        "\n",
        "    cnn_lstm_model.add(Conv1D(filters=64, kernel_size=2, strides=1, padding='valid'))\n",
        "    cnn_lstm_model.add(MaxPooling1D(pool_size=1, strides=2))\n",
        "    # cnn_lstm_model.add(MaxPooling1D(pool_size=1, strides=2))\n",
        "\n",
        "    cnn_lstm_model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "    cnn_lstm_model.add(Dropout(0.2))\n",
        "    cnn_lstm_model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "    cnn_lstm_model.add(Dropout(0.2))\n",
        "\n",
        "    cnn_lstm_model.add(Dense(32, activation='relu'))\n",
        "\n",
        "\n",
        "    cnn_lstm_model.add(Dense(trainY.shape[2], activation='relu'))\n",
        "\n",
        "    # cnn_lstm_model.build(input_shape=(trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
        "\n",
        "    cnn_lstm_model.compile(optimizer='adam', loss='mse')\n",
        "    cnn_lstm_model.summary()\n",
        "    return cnn_lstm_model"
      ],
      "metadata": {
        "id": "N2EV22nTMcC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Conv1D,Conv2D,MaxPooling2D,MaxPooling1D,Flatten\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM,Bidirectional\n",
        "from keras.layers import Dense, Dropout\n",
        "seed=42\n",
        "\n",
        "cnn_lstm_model_without_twitter=build_model((X_train_lstm_without_twitter.shape[1],X_train_lstm_without_twitter.shape[2]))\n",
        "cnn_lstm_model_twitter=build_model((X_train_lstm_twitter.shape[1],X_train_lstm_twitter.shape[2]))\n",
        "\n",
        "history_without_twitter = cnn_lstm_model_without_twitter.fit(X_train_lstm_without_twitter, y_train_lstm_without_twitter, epochs=50, batch_size=64, validation_data=(X_val_lstm_without_twitter, y_val_lstm_without_twitter), verbose=1, )\n",
        "\n",
        "\n",
        "history_twitter = cnn_lstm_model_twitter.fit(X_train_lstm_twitter, y_train_lstm_twitter, epochs=50, batch_size=64, validation_data=(X_val_lstm_twitter, y_val_lstm_twitter), verbose=1, )"
      ],
      "metadata": {
        "id": "xY2dhbXSMki5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(history_without_twitter.history['loss'], label='Training loss')\n",
        "plt.plot(history_without_twitter.history['val_loss'], label='Validation loss')\n",
        "plt.title('Training loss Vs. Validation loss without twitter sentiment analysis')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "3KJnc6NCNTQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(history_twitter.history['loss'], label='Training loss')\n",
        "plt.plot(history_twitter.history['val_loss'], label='Validation loss')\n",
        "plt.title('Training loss Vs. Validation loss including twitter sentiment analysis')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "kSiadcFaNUbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
        "from math import sqrt\n",
        "\n",
        "def plot_predictions_with_dates (type,twitter,dates,y_actual_lstm,y_pred_lstm):\n",
        "    predicted_features=['Open','Adj Close']\n",
        "    for i,predicted_feature in enumerate(predicted_features):\n",
        "        plt.figure(figsize=(15,6))\n",
        "        if twitter :\n",
        "            plt.title(f'LSTM {type} prediction of {predicted_feature} feature After adding twitter sentiment analysis')\n",
        "        else:\n",
        "            plt.title(f'LSTM {type} prediction of {predicted_feature} feature without twitter sentiment analysis')\n",
        "        sns.lineplot(x=dates, y=y_actual_lstm[:,i],label='Actual')\n",
        "        sns.lineplot(x=dates, y=y_pred_lstm[:, i], label='Predicted')\n",
        "        plt.show()\n",
        "        error=mean_squared_error(y_actual_lstm[:,i], y_pred_lstm[:, i])\n",
        "        print(f'Mean square error for {predicted_feature} ={error}')\n",
        "    print('Total mean square error', mean_squared_error(y_actual_lstm, y_pred_lstm))"
      ],
      "metadata": {
        "id": "z1m_3O2dP3PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dates= df_for_training.index[:X_train_lstm_without_twitter.shape[0]]\n",
        "#Make prediction\n",
        "training_prediction_without_twitter = cnn_lstm_model_without_twitter.predict(X_train_lstm_without_twitter)\n",
        "\n",
        "training_prediction_twitter = cnn_lstm_model_twitter.predict(X_train_lstm_twitter)\n",
        "\n",
        "training_prediction_without_twitter=training_prediction_without_twitter.reshape(training_prediction_without_twitter.shape[0], training_prediction_without_twitter.shape[2])\n",
        "\n",
        "training_prediction_twitter=training_prediction_twitter.reshape(training_prediction_twitter.shape[0], training_prediction_twitter.shape[2])\n",
        "\n",
        "y_train_pred_lstm_without_twitter = scaler_for_inference.inverse_transform(training_prediction_without_twitter)\n",
        "\n",
        "y_train_pred_lstm_twitter = scaler_for_inference.inverse_transform(training_prediction_twitter)\n",
        "\n",
        "y_train_lstm_reshaped_without_twitter=y_train_lstm_without_twitter.reshape(y_train_lstm_without_twitter.shape[0], y_train_lstm_without_twitter.shape[2])\n",
        "\n",
        "y_train_actual_lstm = scaler_for_inference.inverse_transform(y_train_lstm_reshaped_without_twitter)"
      ],
      "metadata": {
        "id": "j-ZMd2CWP5qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plot_predictions_with_dates('Training',False,training_dates,y_train_actual_lstm,y_train_pred_lstm_without_twitter)"
      ],
      "metadata": {
        "id": "EMHsayLTP-Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions_with_dates('Training',1,training_dates,y_train_actual_lstm,y_train_pred_lstm_twitter)"
      ],
      "metadata": {
        "id": "DvWGRGg_Q_fz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}